{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","mount_file_id":"1cKti9rJb0aWgTSw1rORUhsO6GnWbg5nd","authorship_tag":"ABX9TyNuQQMXj2a1VXA63xijfSmV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#SimCLR Pretraining 10 000"],"metadata":{"id":"SrjIcH-wuGNb"}},{"cell_type":"code","source":["import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","\n","# Load MobileNetV2 without pretrained weights and remove classifier\n","base_model = mobilenet_v2(weights=None)\n","\n","# Remove the classification head and keep only the feature extractor\n","# The feature output size is 1280 for MobileNetV2\n","backbone = nn.Sequential(\n","    base_model.features,\n","    nn.AdaptiveAvgPool2d(1),  # Ensure consistent output shape\n","    nn.Flatten(),             # Shape: [B, 1280]\n",")\n"],"metadata":{"id":"2M17ehZFCu9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SimCLRModel(nn.Module):\n","    def __init__(self, backbone, feature_dim=128):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(1280, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, feature_dim)\n","        )\n","        #simCLR requires a projection head after the encoder (backbone)\n","\n","    def forward(self, x):\n","        features = self.backbone(x)         # Shape: [B, 1280]\n","        projections = self.projection_head(features)  # Shape: [B, feature_dim]\n","        return projections\n"],"metadata":{"id":"-y8uR2heDKsB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install lightly\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764064909518,"user_tz":-420,"elapsed":8525,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"891d16d9-2a28-40df-b2a3-de8f48bc8f73","collapsed":true,"id":"cPWsuG68ff9-"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightly\n","  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly) (2025.11.12)\n","Collecting hydra-core>=1.0.0 (from lightly)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting lightly_utils~=0.0.0 (from lightly)\n","  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.0.2)\n","Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0.post0)\n","Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.32.4)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly) (1.17.0)\n","Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from lightly) (0.24.0+cu126)\n","Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.11.10)\n","Collecting pytorch_lightning>=1.0.4 (from lightly)\n","  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.5.0)\n","Collecting aenum>=3.1.11 (from lightly)\n","  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (25.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from lightly_utils~=0.0.0->lightly) (11.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.4.2)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning>=1.0.4->lightly) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2025.3.0)\n","Collecting torchmetrics>0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (3.13.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->lightly) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->lightly) (3.0.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.22.0)\n","Downloading lightly-1.5.22-py3-none-any.whl (859 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n","Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: aenum, lightning-utilities, lightly_utils, hydra-core, torchmetrics, pytorch_lightning, lightly\n","Successfully installed aenum-3.1.16 hydra-core-1.3.2 lightly-1.5.22 lightly_utils-0.0.2 lightning-utilities-0.15.2 pytorch_lightning-2.5.6 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","source":["from lightly.data import LightlyDataset\n","from lightly.transforms import SimCLRTransform\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder"],"metadata":{"id":"xq3hiAt-_EV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SimCLR Transform from lightly\n","simclr_transform = SimCLRTransform(input_size=224)  # you can change input_size as needed\n","\n","# Custom dataset to return 2 views\n","class SimCLRDataset(ImageFolder):\n","    def __getitem__(self, index):\n","        sample, _ = super().__getitem__(index)\n","        xi, xj = simclr_transform(sample)\n","        return xi, xj\n"],"metadata":{"id":"7_pBY_qIxVjD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/pets0/unlabeled_train'\n","\n","# Create dataset and dataloader\n","dataset = SimCLRDataset(root=data_path)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n","num_imgs = len(dataset)\n","print(f\"Number of images in the dataset: {num_imgs}\")"],"metadata":{"id":"u2-poWYsg9N9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764064998771,"user_tz":-420,"elapsed":21401,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"fec964c3-9504-43de-db98-6e5691effbeb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in the dataset: 10000\n"]}]},{"cell_type":"code","source":["from lightly.loss import NTXentLoss\n","import torch\n","\n","# Initialize model\n","model = SimCLRModel(backbone)\n","model = model.cuda()\n","\n","# Loss\n","criterion = NTXentLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","# Training loop\n","for epoch in range(20):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcd2c083-a7ee-48ef-d47f-09621baa1ab3","id":"9fGModtegazj","executionInfo":{"status":"ok","timestamp":1764070628971,"user_tz":-420,"elapsed":5614318,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Loss = 3.1890\n","Epoch 1: Loss = 3.3769\n","Epoch 2: Loss = 2.8553\n","Epoch 3: Loss = 3.3284\n","Epoch 4: Loss = 2.7613\n","Epoch 5: Loss = 2.7699\n","Epoch 6: Loss = 2.6084\n","Epoch 7: Loss = 2.7445\n","Epoch 8: Loss = 2.5992\n","Epoch 9: Loss = 2.7181\n","Epoch 10: Loss = 2.5164\n","Epoch 11: Loss = 2.6766\n","Epoch 12: Loss = 2.5250\n","Epoch 13: Loss = 2.5185\n","Epoch 14: Loss = 2.6390\n","Epoch 15: Loss = 2.5899\n","Epoch 16: Loss = 2.6170\n","Epoch 17: Loss = 2.4712\n","Epoch 18: Loss = 2.3676\n","Epoch 19: Loss = 2.8177\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth')"],"metadata":{"id":"M4XAtivfeJXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(5):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","    print(f\"Epoch {epoch+20}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMhDkuDQDNTA","executionInfo":{"status":"ok","timestamp":1764072808395,"user_tz":-420,"elapsed":1056507,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"2c80701b-34ca-4003-d0d6-4b54b321d454"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 2.4320\n","Epoch 21: Loss = 2.4563\n","Epoch 22: Loss = 2.4456\n","Epoch 23: Loss = 2.5486\n","Epoch 24: Loss = 2.5704\n"]}]},{"cell_type":"code","source":["# Training loop\n","mloss = 2.5\n","for epoch in range(5):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if mloss > loss.item():\n","          mloss = loss.item()\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth')\n","\n","    print(f\"Epoch {epoch+25}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764074045843,"user_tz":-420,"elapsed":1107904,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"245e32e6-8668-41bf-a45b-7aa2dd543bc2","id":"MudN3SVrKpwG"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 25: Loss = 2.5670\n","Epoch 26: Loss = 2.2463\n","Epoch 27: Loss = 2.3873\n","Epoch 28: Loss = 2.2702\n","Epoch 29: Loss = 2.4605\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth')"],"metadata":{"id":"5xum0-tWDdKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","    if mloss > loss.item():\n","          mloss = loss.item()\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth')\n","\n","    print(f\"Epoch {epoch+30}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764076385466,"user_tz":-420,"elapsed":2112688,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"3dcc20d5-1fe2-4d9b-d5ed-20a4770c9b89","id":"JjMyzDCySpss"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 30: Loss = 2.3099\n","Epoch 31: Loss = 2.4192\n","Epoch 32: Loss = 2.5080\n","Epoch 33: Loss = 2.3715\n","Epoch 34: Loss = 2.1255\n","Epoch 35: Loss = 2.3541\n","Epoch 36: Loss = 2.3326\n","Epoch 37: Loss = 2.1406\n","Epoch 38: Loss = 2.3493\n","Epoch 39: Loss = 2.1011\n"]}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","\n","    if mloss > loss.item():\n","          mloss = loss.item()\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth')\n","\n","    print(f\"Epoch {epoch+40}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764078569195,"user_tz":-420,"elapsed":2116997,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"7b358421-5e3f-4f15-b2f6-25e538e51ab0","id":"tHCln8mRJhxM"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 40: Loss = 2.3627\n","Epoch 41: Loss = 2.2122\n","Epoch 42: Loss = 2.0049\n","Epoch 43: Loss = 2.2572\n","Epoch 44: Loss = 2.3366\n","Epoch 45: Loss = 2.3494\n","Epoch 46: Loss = 2.1856\n","Epoch 47: Loss = 2.1016\n","Epoch 48: Loss = 2.0798\n","Epoch 49: Loss = 2.3723\n"]}]},{"cell_type":"markdown","source":["####Continue training (hit GPU limit on colab)"],"metadata":{"id":"G6r-7opKvMZy"}},{"cell_type":"code","source":["!pip install lightly\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764235661472,"user_tz":-420,"elapsed":7824,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"0289c56b-5929-4e75-fdd5-539a08ab50f5","collapsed":true,"id":"6f9lTjI9vMZz"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightly\n","  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly) (2025.11.12)\n","Collecting hydra-core>=1.0.0 (from lightly)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting lightly_utils~=0.0.0 (from lightly)\n","  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.0.2)\n","Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0.post0)\n","Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.32.4)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly) (1.17.0)\n","Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from lightly) (0.24.0+cu126)\n","Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.12.3)\n","Collecting pytorch_lightning>=1.0.4 (from lightly)\n","  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.5.0)\n","Collecting aenum>=3.1.11 (from lightly)\n","  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (25.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from lightly_utils~=0.0.0->lightly) (11.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (2.41.4)\n","Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.4.2)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning>=1.0.4->lightly) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2025.3.0)\n","Collecting torchmetrics>0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (3.13.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->lightly) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->lightly) (3.0.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.22.0)\n","Downloading lightly-1.5.22-py3-none-any.whl (859 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n","Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: aenum, lightning-utilities, lightly_utils, hydra-core, torchmetrics, pytorch_lightning, lightly\n","Successfully installed aenum-3.1.16 hydra-core-1.3.2 lightly-1.5.22 lightly_utils-0.0.2 lightning-utilities-0.15.2 pytorch_lightning-2.5.6 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","\n","# Load MobileNetV2 without pretrained weights and remove classifier\n","base_model = mobilenet_v2(weights=None)\n","\n","# Remove the classification head and keep only the feature extractor\n","# The feature output size is 1280 for MobileNetV2\n","backbone = nn.Sequential(\n","    base_model.features,\n","    nn.AdaptiveAvgPool2d(1),  # Ensure consistent output shape\n","    nn.Flatten(),             # Shape: [B, 1280]\n",")\n"],"metadata":{"id":"PjuBkOZ0vMZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SimCLRModel(nn.Module):\n","    def __init__(self, backbone, feature_dim=128):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(1280, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, feature_dim)\n","        )\n","\n","    def forward(self, x):\n","        features = self.backbone(x)         # Shape: [B, 1280]\n","        projections = self.projection_head(features)  # Shape: [B, feature_dim]\n","        return projections\n"],"metadata":{"id":"S7d7oVgEvMZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightly.data import LightlyDataset\n","from lightly.transforms import SimCLRTransform\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder"],"metadata":{"id":"il1TIGiLvMZ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764235699691,"user_tz":-420,"elapsed":2311,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"5718cac8-d23d-462d-e98d-c974d2999083","id":"FG98sP9_vMZ0"},"source":["import torch\n","# Define the path to your saved pretrained student model checkpoint\n","pretrained_student_path = '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth'\n","\n","# Load the state dictionary from the saved checkpoint\n","# Using map_location='cpu' to load onto CPU first is safer, then move to device\n","student_state_dict = torch.load(pretrained_student_path, map_location='cpu')\n","\n","# Create the student model architecture\n","# Assuming the student model is SimCLRModel(backbone) as defined previously\n","# If your model architecture is different, adjust this part accordingly\n","model = SimCLRModel(backbone)\n","\n","# Load the state dictionary into the model\n","# Use strict=True if the saved state_dict exactly matches the current model structure\n","# Use strict=False if there are missing or extra keys (e.g., if you saved the entire model instead of just the state_dict)\n","model.load_state_dict(student_state_dict, strict=True)\n","\n","# Move the model to the appropriate device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","print(\"Pretrained student model loaded successfully.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained student model loaded successfully.\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/pets0/unlabeled_train'\n","# SimCLR Transform from lightly\n","simclr_transform = SimCLRTransform(input_size=224)  # you can change input_size as needed\n","\n","# Custom dataset to return 2 views\n","class SimCLRDataset(ImageFolder):\n","    def __getitem__(self, index):\n","        sample, _ = super().__getitem__(index)\n","        xi, xj = simclr_transform(sample)\n","        return xi, xj\n","\n","# Create dataset and dataloader\n","dataset = SimCLRDataset(root=data_path)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n","num_imgs = len(dataset)\n","print(f\"Number of images in the dataset: {num_imgs}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764235728972,"user_tz":-420,"elapsed":22289,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"6285b6e3-8a16-4ea7-be6f-daca1aa52f13","id":"mQLpyHNKvMZ0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in the dataset: 10000\n"]}]},{"cell_type":"code","source":["from lightly.loss import NTXentLoss\n","import torch\n","\n","# Loss\n","criterion = NTXentLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","mloss = 2.0\n","# Training loop\n","for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if mloss > loss.item():\n","          mloss = loss.item()\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_cont.pth')\n","\n","    if (epoch+1) % 5 == 0:\n","      torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+43}: Loss = {loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"295848cd-8556-4170-85fe-b8b031250e50","id":"Bh9W4MqEvMZ0","executionInfo":{"status":"ok","timestamp":1764241233472,"user_tz":-420,"elapsed":5403757,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 43: Loss = 2.2164\n","Epoch 44: Loss = 2.3832\n","Epoch 45: Loss = 2.2882\n","Epoch 46: Loss = 2.1571\n","Epoch 47: Loss = 2.3185\n","Epoch 48: Loss = 2.1638\n","Epoch 49: Loss = 2.0395\n","Epoch 50: Loss = 2.0959\n","Epoch 51: Loss = 1.9750\n","Epoch 52: Loss = 2.1218\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch.pth')"],"metadata":{"id":"I0ek9_HW-fbF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(8):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if mloss > loss.item():\n","          mloss = loss.item()\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_cont.pth')\n","\n","    if (epoch+1) % 5 == 0:\n","      torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+53}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGxBwj4U-k4k","outputId":"936e2ec0-62de-4808-a0d0-33f78f4af813"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 53: Loss = 1.9999\n","Epoch 54: Loss = 2.2887\n","Epoch 55: Loss = 2.1144\n","Epoch 56: Loss = 2.1746\n","Epoch 57: Loss = 2.1592\n","Epoch 58: Loss = 2.0773\n","Epoch 59: Loss = 2.4056\n"]}]},{"cell_type":"markdown","source":["####Continue training 2"],"metadata":{"id":"uIx0Yhwlan7U"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764299122800,"user_tz":-420,"elapsed":2696,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"7d8fceba-8280-48e4-fd24-6c6d32836c3b","id":"usxFDRoTan7X"},"source":["import torch\n","# Define the path to your saved pretrained student model checkpoint\n","pretrained_student_path = '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch.pth'\n","\n","# Load the state dictionary from the saved checkpoint\n","# Using map_location='cpu' to load onto CPU first is safer, then move to device\n","student_state_dict = torch.load(pretrained_student_path, map_location='cpu')\n","\n","# Create the student model architecture\n","# Assuming the student model is SimCLRModel(backbone) as defined previously\n","# If your model architecture is different, adjust this part accordingly\n","model = SimCLRModel(backbone)\n","\n","# Load the state dictionary into the model\n","# Use strict=True if the saved state_dict exactly matches the current model structure\n","# Use strict=False if there are missing or extra keys (e.g., if you saved the entire model instead of just the state_dict)\n","model.load_state_dict(student_state_dict, strict=True)\n","\n","# Move the model to the appropriate device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","print(\"Pretrained student model loaded successfully.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained student model loaded successfully.\n"]}]},{"cell_type":"code","source":["from lightly.loss import NTXentLoss\n","import torch\n","\n","# Loss\n","criterion = NTXentLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","mloss = 1.97\n","# Training loop\n","for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if mloss > loss.item():\n","          mloss = loss.item()\n","          torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_cont2.pth')\n","\n","    if (epoch+1) % 2 == 0:\n","      torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch2.pth')\n","\n","    print(f\"Epoch {epoch+58}: Loss = {loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"69d18d87-c1f0-40c2-c4d3-dbf9b8b20171","executionInfo":{"status":"ok","timestamp":1764302592325,"user_tz":-420,"elapsed":3341299,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"id":"cxuTjULdan7Y"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 58: Loss = 2.2098\n","Epoch 59: Loss = 2.3115\n","Epoch 60: Loss = 2.3314\n","Epoch 61: Loss = 2.3468\n","Epoch 62: Loss = 2.0173\n","Epoch 63: Loss = 2.1298\n","Epoch 64: Loss = 2.2794\n","Epoch 65: Loss = 2.2211\n","Epoch 66: Loss = 2.2764\n","Epoch 67: Loss = 2.2217\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch2.pth')"],"metadata":{"id":"sBgp2PhAan7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4df99b28"},"source":["# Finetune the pretrained student model 10 000"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms, datasets"],"metadata":{"id":"c8EZhOn0N2aG"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6a0e79cc","executionInfo":{"status":"ok","timestamp":1764144874074,"user_tz":-420,"elapsed":2400,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"e2e2e93d-f49f-4640-e223-d6fa412a13ea"},"source":["# Define transforms for finetuning\n","\n","finetune_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.CenterCrop(192),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Create datasets and dataloaders for finetuning\n","finetune_dataset_labeled = ImageFolder('/content/drive/MyDrive/pets0/finetune_train', transform=finetune_transform)\n","val_dataset_labeled = ImageFolder('/content/drive/MyDrive/pets0/val', transform=val_transform)\n","\n","finetune_loader_labeled = DataLoader(finetune_dataset_labeled, batch_size=64, shuffle=True, num_workers=2)\n","val_loader_labeled = DataLoader(val_dataset_labeled, batch_size=64, shuffle=False, num_workers=2)\n","\n","print(f\"Number of samples in finetune dataset: {len(finetune_dataset_labeled)}\")\n","print(f\"Number of samples in validation dataset: {len(val_dataset_labeled)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples in finetune dataset: 420\n","Number of samples in validation dataset: 180\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7f1b7e11","executionInfo":{"status":"ok","timestamp":1764144947321,"user_tz":-420,"elapsed":368,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"11075609-5939-4a4e-d912-67e7c8feed0e"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Load the pretrained SimCLR model state dict\n","pretrained_path = '/content/drive/MyDrive/mods/mobilenet_sim_10000.pth'\n","simclr_state_dict = torch.load(pretrained_path, map_location=device)\n","\n","# Create a standard MobileNetV2 model\n","student_finetune = mobilenet_v2(weights=None)\n","\n","# Filter the state dict to keep only the backbone weights\n","backbone_state_dict = {}\n","for k, v in simclr_state_dict.items():\n","    # Keys in the saved SimCLRModel state dict for the backbone start with 'backbone.0.'\n","    # We want to load these into the 'features.' of the standard MobileNetV2\n","    if k.startswith('backbone.0.'):\n","        backbone_state_dict[k.replace('backbone.0.', 'features.')] = v\n","    # Also handle the case if the state dict keys were just 'backbone.' without the '0.'\n","    elif k.startswith('backbone.'):\n","        backbone_state_dict[k.replace('backbone.', 'features.')] = v\n","\n","\n","# Load the backbone weights into the standard MobileNetV2 model\n","# Use strict=False because we are not loading the classifier weights\n","student_finetune.load_state_dict(backbone_state_dict, strict=False)\n","\n","# Replace the classifier\n","num_ftrs = student_finetune.classifier[1].in_features\n","student_finetune.classifier[1] = nn.Linear(num_ftrs, 2) # 2 classes: cat, dog\n","\n","student_finetune = student_finetune.to(device)\n","\n","# Optionally freeze the backbone and only train the classifier for faster initial training\n","# for param in student_finetune.features.parameters():\n","#     param.requires_grad = False\n","\n","print(\"Pretrained MobileNetV2 loaded and classifier replaced.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained MobileNetV2 loaded and classifier replaced.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8da5db8","executionInfo":{"status":"ok","timestamp":1764145233739,"user_tz":-420,"elapsed":177202,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"9cef42ab-17f6-4814-a859-ebbba8df4e16"},"source":["# Define optimizer and loss function for finetuning\n","optimizer_finetune = torch.optim.Adam(student_finetune.parameters(), lr=1e-4) # Start with a lower learning rate\n","criterion_finetune = nn.CrossEntropyLoss()\n","\n","# Training loop\n","for epoch in range(10):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    print(f'Epoch {epoch+1}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss: 0.6793 Acc: 0.5762 | Val Loss: 0.6300 Acc: 0.7444\n","Epoch 2: Train Loss: 0.6182 Acc: 0.6952 | Val Loss: 0.5751 Acc: 0.7611\n","Epoch 3: Train Loss: 0.5733 Acc: 0.7238 | Val Loss: 0.5383 Acc: 0.7722\n","Epoch 4: Train Loss: 0.5214 Acc: 0.7690 | Val Loss: 0.5085 Acc: 0.8000\n","Epoch 5: Train Loss: 0.4980 Acc: 0.7762 | Val Loss: 0.4862 Acc: 0.7778\n","Epoch 6: Train Loss: 0.4440 Acc: 0.8071 | Val Loss: 0.4735 Acc: 0.7778\n","Epoch 7: Train Loss: 0.4241 Acc: 0.8119 | Val Loss: 0.4632 Acc: 0.7778\n","Epoch 8: Train Loss: 0.4033 Acc: 0.8119 | Val Loss: 0.4855 Acc: 0.7500\n","Epoch 9: Train Loss: 0.3614 Acc: 0.8405 | Val Loss: 0.4933 Acc: 0.7500\n","Epoch 10: Train Loss: 0.3428 Acc: 0.8571 | Val Loss: 0.4808 Acc: 0.7444\n"]}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(6):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    print(f'Epoch {epoch+11}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-9zv7ODIQHEI","executionInfo":{"status":"ok","timestamp":1764145331395,"user_tz":-420,"elapsed":37372,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"18102fff-1ecc-41e7-94e4-9982500f1f63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 11: Train Loss: 0.3278 Acc: 0.8595 | Val Loss: 0.4756 Acc: 0.7333\n","Epoch 12: Train Loss: 0.3610 Acc: 0.8476 | Val Loss: 0.4870 Acc: 0.7500\n","Epoch 13: Train Loss: 0.2893 Acc: 0.8810 | Val Loss: 0.4996 Acc: 0.7667\n","Epoch 14: Train Loss: 0.2752 Acc: 0.8810 | Val Loss: 0.5087 Acc: 0.7556\n","Epoch 15: Train Loss: 0.2533 Acc: 0.9000 | Val Loss: 0.5000 Acc: 0.7556\n","Epoch 16: Train Loss: 0.2369 Acc: 0.9190 | Val Loss: 0.4840 Acc: 0.8000\n"]}]},{"cell_type":"code","source":["torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_finetuned_10000.pth')"],"metadata":{"id":"gPkMzzEcPG7Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(5):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    print(f'Epoch {epoch+17}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EAqGQRGlQaFK","executionInfo":{"status":"ok","timestamp":1764145391458,"user_tz":-420,"elapsed":31060,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"f6498d32-26e7-444c-a82c-13f4cc6b3c64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 17: Train Loss: 0.2108 Acc: 0.9286 | Val Loss: 0.4865 Acc: 0.7889\n","Epoch 18: Train Loss: 0.1957 Acc: 0.9286 | Val Loss: 0.5109 Acc: 0.7778\n","Epoch 19: Train Loss: 0.1863 Acc: 0.9452 | Val Loss: 0.5302 Acc: 0.7167\n","Epoch 20: Train Loss: 0.1732 Acc: 0.9405 | Val Loss: 0.5250 Acc: 0.7556\n","Epoch 21: Train Loss: 0.1626 Acc: 0.9476 | Val Loss: 0.5326 Acc: 0.7722\n"]}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(5):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    print(f'Epoch {epoch+22}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764147735900,"user_tz":-420,"elapsed":30603,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"fb7bd294-4fd7-4069-f9cf-39e00d673073","id":"_tVCy6JjZWn-"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 22: Train Loss: 0.1385 Acc: 0.9595 | Val Loss: 0.5486 Acc: 0.7667\n","Epoch 23: Train Loss: 0.1165 Acc: 0.9714 | Val Loss: 0.5918 Acc: 0.7222\n","Epoch 24: Train Loss: 0.1133 Acc: 0.9667 | Val Loss: 0.5718 Acc: 0.7500\n","Epoch 25: Train Loss: 0.0965 Acc: 0.9643 | Val Loss: 0.6047 Acc: 0.7500\n","Epoch 26: Train Loss: 0.0917 Acc: 0.9738 | Val Loss: 0.6562 Acc: 0.7556\n"]}]},{"cell_type":"code","source":["torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_over_finetuned_10000.pth')"],"metadata":{"id":"BDsC-xKTZqt_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(5):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    print(f'Epoch {epoch+27}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764148015390,"user_tz":-420,"elapsed":38227,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"835fd144-536d-4aa6-a742-6cf27670b688","id":"GGs7cAlQaauN"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 27: Train Loss: 0.1007 Acc: 0.9738 | Val Loss: 0.6235 Acc: 0.7556\n","Epoch 28: Train Loss: 0.0767 Acc: 0.9857 | Val Loss: 0.6155 Acc: 0.7611\n","Epoch 29: Train Loss: 0.0858 Acc: 0.9762 | Val Loss: 0.6313 Acc: 0.7778\n","Epoch 30: Train Loss: 0.0622 Acc: 0.9857 | Val Loss: 0.6276 Acc: 0.7833\n","Epoch 31: Train Loss: 0.0615 Acc: 0.9905 | Val Loss: 0.6389 Acc: 0.7667\n"]}]},{"cell_type":"code","source":["torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_more_over_finetuned_10000.pth')"],"metadata":{"id":"e7A97iyiaho9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#SimCLR Pretraining 3000"],"metadata":{"id":"tdeWwJ2TXE4W"}},{"cell_type":"code","source":["!pip install lightly\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764300024990,"user_tz":-420,"elapsed":14199,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"8d4a7a83-b06a-4471-a9b0-57ed53e1eb08","collapsed":true,"id":"-GPRznTMXE4W"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightly\n","  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly) (2025.11.12)\n","Collecting hydra-core>=1.0.0 (from lightly)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting lightly_utils~=0.0.0 (from lightly)\n","  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.0.2)\n","Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0.post0)\n","Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.32.4)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly) (1.17.0)\n","Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from lightly) (0.24.0+cu126)\n","Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.12.3)\n","Collecting pytorch_lightning>=1.0.4 (from lightly)\n","  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.5.0)\n","Collecting aenum>=3.1.11 (from lightly)\n","  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (25.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from lightly_utils~=0.0.0->lightly) (11.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (2.41.4)\n","Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.4.2)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning>=1.0.4->lightly) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2025.3.0)\n","Collecting torchmetrics>0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (3.13.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->lightly) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->lightly) (3.0.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.22.0)\n","Downloading lightly-1.5.22-py3-none-any.whl (859 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n","Downloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: aenum, lightning-utilities, lightly_utils, hydra-core, torchmetrics, pytorch_lightning, lightly\n","Successfully installed aenum-3.1.16 hydra-core-1.3.2 lightly-1.5.22 lightly_utils-0.0.2 lightning-utilities-0.15.2 pytorch_lightning-2.5.6 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","source":["import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","\n","# Load MobileNetV2 without pretrained weights and remove classifier\n","base_model = mobilenet_v2(weights=None)\n","\n","# Remove the classification head and keep only the feature extractor\n","# The feature output size is 1280 for MobileNetV2\n","backbone = nn.Sequential(\n","    base_model.features,\n","    nn.AdaptiveAvgPool2d(1),  # Ensure consistent output shape\n","    nn.Flatten(),             # Shape: [B, 1280]\n",")\n"],"metadata":{"id":"Zw6qUHD6XE4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SimCLRModel(nn.Module):\n","    def __init__(self, backbone, feature_dim=128):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(1280, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, feature_dim)\n","        )\n","\n","    def forward(self, x):\n","        features = self.backbone(x)         # Shape: [B, 1280]\n","        projections = self.projection_head(features)  # Shape: [B, feature_dim]\n","        return projections\n"],"metadata":{"id":"qfUL10NzXE4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from lightly.data import LightlyDataset\n","from lightly.transforms import SimCLRTransform\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder"],"metadata":{"id":"7O6G_NdDXE4X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/pets0/train3000'\n","# SimCLR Transform from lightly\n","simclr_transform = SimCLRTransform(input_size=224)  # you can change input_size as needed\n","\n","# Custom dataset to return 2 views\n","class SimCLRDataset(ImageFolder):\n","    def __getitem__(self, index):\n","        sample, _ = super().__getitem__(index)\n","        xi, xj = simclr_transform(sample)\n","        return xi, xj\n","\n","# Create dataset and dataloader\n","dataset = SimCLRDataset(root=data_path)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2)\n","num_imgs = len(dataset)\n","print(f\"Number of images in the dataset: {num_imgs}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764300204334,"user_tz":-420,"elapsed":5303,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"46ac930c-2f2b-4aaa-a514-6fc1bb7f0f47","id":"lUWSujvYXE4X"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in the dataset: 3000\n"]}]},{"cell_type":"code","source":["from lightly.loss import NTXentLoss\n","import torch\n","\n","# Initialize model\n","model = SimCLRModel(backbone)\n","model = model.cuda()\n","\n","# Loss\n","criterion = NTXentLoss()\n","\n","# Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n","\n","mloss = 10.0\n","# Training loop\n","for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae5ee20a-fc43-4099-bb6a-23febf2580f1","executionInfo":{"status":"ok","timestamp":1764301140526,"user_tz":-420,"elapsed":849720,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"id":"AqRNJ-rUXE4Y"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: Loss = 4.6568\n","Epoch 1: Loss = 4.5776\n","Epoch 2: Loss = 4.5338\n","Epoch 3: Loss = 4.4639\n","Epoch 4: Loss = 4.3600\n","Epoch 5: Loss = 4.4496\n","Epoch 6: Loss = 4.4015\n","Epoch 7: Loss = 4.2809\n","Epoch 8: Loss = 4.5226\n","Epoch 9: Loss = 4.2381\n"]}]},{"cell_type":"code","source":["for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+10}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEI53hZKlwF4","executionInfo":{"status":"ok","timestamp":1764302791997,"user_tz":-420,"elapsed":627733,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"3cb4bb31-5ee1-4114-8ebe-ff2e889d0c68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 10: Loss = 4.2056\n","Epoch 11: Loss = 4.1428\n","Epoch 12: Loss = 4.2731\n","Epoch 13: Loss = 4.2184\n","Epoch 14: Loss = 4.1554\n","Epoch 15: Loss = 4.1283\n","Epoch 16: Loss = 3.8937\n","Epoch 17: Loss = 4.2686\n","Epoch 18: Loss = 4.0384\n","Epoch 19: Loss = 3.9816\n"]}]},{"cell_type":"code","source":["for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+20}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764303606185,"user_tz":-420,"elapsed":622772,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"ea4d8002-a236-4410-8adf-4644c15b2ac0","id":"JtNvlnobpoVJ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 20: Loss = 3.9605\n","Epoch 21: Loss = 3.8694\n","Epoch 22: Loss = 3.9110\n","Epoch 23: Loss = 3.8154\n","Epoch 24: Loss = 4.1257\n","Epoch 25: Loss = 3.9339\n","Epoch 26: Loss = 3.8907\n","Epoch 27: Loss = 3.8074\n","Epoch 28: Loss = 3.8780\n","Epoch 29: Loss = 3.9567\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')"],"metadata":{"id":"4mKBoV3hptgf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+30}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764304849052,"user_tz":-420,"elapsed":626212,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"14e63c00-d61b-41bb-cb8f-cb38f2f82dea","id":"0g44NSgBudA2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 30: Loss = 3.9850\n","Epoch 31: Loss = 3.9224\n","Epoch 32: Loss = 3.9892\n","Epoch 33: Loss = 3.7859\n","Epoch 34: Loss = 3.8417\n","Epoch 35: Loss = 3.9339\n","Epoch 36: Loss = 3.7608\n","Epoch 37: Loss = 3.8823\n","Epoch 38: Loss = 3.8279\n","Epoch 39: Loss = 3.8465\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bKCoLOXPxBAt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+40}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764305528240,"user_tz":-420,"elapsed":623103,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"4458ced9-7181-4d8d-fa0f-e20eb7235851","id":"ZhN6IpHkxBsL"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 40: Loss = 3.9658\n","Epoch 41: Loss = 3.7096\n","Epoch 42: Loss = 3.7941\n","Epoch 43: Loss = 3.9225\n","Epoch 44: Loss = 3.7813\n","Epoch 45: Loss = 3.8362\n","Epoch 46: Loss = 3.7475\n","Epoch 47: Loss = 3.8510\n","Epoch 48: Loss = 3.7701\n","Epoch 49: Loss = 3.6677\n"]}]},{"cell_type":"code","source":["for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+50}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764306635157,"user_tz":-420,"elapsed":614088,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"197400e7-2503-4bc1-a8d3-7130ab9ccd52","id":"p5w_av8b1SoG"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 50: Loss = 3.6622\n","Epoch 51: Loss = 3.6666\n","Epoch 52: Loss = 3.8610\n","Epoch 53: Loss = 3.7226\n","Epoch 54: Loss = 3.6765\n","Epoch 55: Loss = 3.7334\n","Epoch 56: Loss = 3.7732\n","Epoch 57: Loss = 3.7030\n","Epoch 58: Loss = 3.7847\n","Epoch 59: Loss = 3.6570\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')"],"metadata":{"id":"TqwVAvFQ4JoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","    for (views) in dataloader:  # Assuming LightlyDataset with SimCLRTransform\n","\n","        view1, view2 = views[0].cuda(), views[1].cuda()\n","\n","\n","        z1 = model(view1)\n","        z2 = model(view2)\n","\n","\n","        loss = criterion(z1, z2)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    if loss < mloss:\n","        mloss = loss.item()\n","        torch.save(model.state_dict(), '/content/drive/MyDrive/mods/mobilenet_sim_3000.pth')\n","    if (epoch+1)%2==0:\n","        torch.save(model.state_dict(),  '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth')\n","\n","    print(f\"Epoch {epoch+60}: Loss = {loss.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e348774-378d-4f61-b853-95592b8125ee","id":"VFTEX4UK4h_x"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 60: Loss = 3.8698\n","Epoch 61: Loss = 3.6273\n","Epoch 62: Loss = 3.6860\n","Epoch 63: Loss = 3.8455\n","Epoch 64: Loss = 3.7100\n"]}]},{"cell_type":"markdown","metadata":{"id":"-iAaWNbTAdjU"},"source":["# Finetune the pretrained student model 3 000"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import ImageFolder\n","from torchvision import transforms, datasets"],"metadata":{"id":"3kUm-uqIAdjV"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764309014012,"user_tz":-420,"elapsed":1606,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"c58baa8d-e146-41e7-f246-b8987b470a0e","id":"mXjRliIzAdjV"},"source":["# Define transforms for finetuning\n","\n","finetune_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.CenterCrop(192),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Create datasets and dataloaders for finetuning\n","finetune_dataset_labeled = ImageFolder('/content/drive/MyDrive/pets0/finetune_train', transform=finetune_transform)\n","val_dataset_labeled = ImageFolder('/content/drive/MyDrive/pets0/val', transform=val_transform)\n","\n","finetune_loader_labeled = DataLoader(finetune_dataset_labeled, batch_size=64, shuffle=True, num_workers=2)\n","val_loader_labeled = DataLoader(val_dataset_labeled, batch_size=64, shuffle=False, num_workers=2)\n","\n","print(f\"Number of samples in finetune dataset: {len(finetune_dataset_labeled)}\")\n","print(f\"Number of samples in validation dataset: {len(val_dataset_labeled)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples in finetune dataset: 420\n","Number of samples in validation dataset: 180\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764309037325,"user_tz":-420,"elapsed":884,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"88219fe4-9508-4219-9bb9-d766e6b2c81c","id":"aJTY1iy0AdjW"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Load the pretrained SimCLR model state dict\n","pretrained_path = '/content/drive/MyDrive/mods/mobilenet_sim_3000_epoch.pth'\n","simclr_state_dict = torch.load(pretrained_path, map_location=device)\n","\n","# Create a standard MobileNetV2 model\n","student_finetune = mobilenet_v2(weights=None)\n","\n","# Filter the state dict to keep only the backbone weights\n","backbone_state_dict = {}\n","for k, v in simclr_state_dict.items():\n","    # Keys in the saved SimCLRModel state dict for the backbone start with 'backbone.0.'\n","    # We want to load these into the 'features.' of the standard MobileNetV2\n","    if k.startswith('backbone.0.'):\n","        backbone_state_dict[k.replace('backbone.0.', 'features.')] = v\n","    # Also handle the case if the state dict keys were just 'backbone.' without the '0.'\n","    elif k.startswith('backbone.'):\n","        backbone_state_dict[k.replace('backbone.', 'features.')] = v\n","\n","\n","# Load the backbone weights into the standard MobileNetV2 model\n","# Use strict=False because we are not loading the classifier weights\n","student_finetune.load_state_dict(backbone_state_dict, strict=False)\n","\n","# Replace the classifier\n","num_ftrs = student_finetune.classifier[1].in_features\n","student_finetune.classifier[1] = nn.Linear(num_ftrs, 2) # 2 classes: cat, dog\n","\n","student_finetune = student_finetune.to(device)\n","\n","# Optionally freeze the backbone and only train the classifier for faster initial training\n","# for param in student_finetune.features.parameters():\n","#     param.requires_grad = False\n","\n","print(\"Pretrained MobileNetV2 loaded and classifier replaced.\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained MobileNetV2 loaded and classifier replaced.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764310221555,"user_tz":-420,"elapsed":1077565,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"d6d39ae1-c90d-4507-c1fc-3e96b1f1fe29","id":"XdiC0cG-AdjW"},"source":["# Define optimizer and loss function for finetuning\n","optimizer_finetune = torch.optim.Adam(student_finetune.parameters(), lr=1e-4) # Start with a lower learning rate\n","criterion_finetune = nn.CrossEntropyLoss()\n","\n","vacc = 0.0\n","# Training loop\n","for epoch in range(10):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    if (epoch+1) % 2 == 0:\n","        torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_finetuned_3000_epoch.pth')\n","    if val_acc > vacc:\n","        torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_finetuned_3000.pth')\n","    print(f'Epoch {epoch+1}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss: 0.7052 Acc: 0.5310 | Val Loss: 0.6311 Acc: 0.6444\n","Epoch 2: Train Loss: 0.6544 Acc: 0.6452 | Val Loss: 0.6014 Acc: 0.7222\n","Epoch 3: Train Loss: 0.6239 Acc: 0.6643 | Val Loss: 0.5766 Acc: 0.7444\n","Epoch 4: Train Loss: 0.5974 Acc: 0.6881 | Val Loss: 0.5604 Acc: 0.7444\n","Epoch 5: Train Loss: 0.5805 Acc: 0.7143 | Val Loss: 0.5467 Acc: 0.7167\n","Epoch 6: Train Loss: 0.5459 Acc: 0.7190 | Val Loss: 0.5339 Acc: 0.7167\n","Epoch 7: Train Loss: 0.5394 Acc: 0.7310 | Val Loss: 0.5253 Acc: 0.7167\n","Epoch 8: Train Loss: 0.4910 Acc: 0.7810 | Val Loss: 0.5305 Acc: 0.7056\n","Epoch 9: Train Loss: 0.5186 Acc: 0.7452 | Val Loss: 0.5385 Acc: 0.7056\n","Epoch 10: Train Loss: 0.4547 Acc: 0.7786 | Val Loss: 0.5296 Acc: 0.7111\n"]}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(10):\n","    student_finetune.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for images, labels in finetune_loader_labeled:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer_finetune.zero_grad()\n","        outputs = student_finetune(images)\n","        loss = criterion_finetune(outputs, labels)\n","        loss.backward()\n","        optimizer_finetune.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = correct / total\n","\n","    # Validation\n","    student_finetune.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader_labeled:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = student_finetune(images)\n","            loss = criterion_finetune(outputs, labels)\n","\n","            val_running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            val_correct += (preds == labels).sum().item()\n","            val_total += labels.size(0)\n","\n","    val_loss = val_running_loss / val_total\n","    val_acc = val_correct / val_total\n","\n","    if (epoch+1) % 2 == 0:\n","        torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_finetuned_3000_epoch.pth')\n","    if val_acc > vacc:\n","        vacc = val_acc\n","        torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_finetuned_3000.pth')\n","    print(f'Epoch {epoch+1}: '\n","          f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","          f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_PNHy6QFkH9","executionInfo":{"status":"ok","timestamp":1764311367466,"user_tz":-420,"elapsed":1016805,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"7a00372d-b418-4e38-dcd8-8bd69edab6ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss: 0.4284 Acc: 0.8214 | Val Loss: 0.5192 Acc: 0.7278\n","Epoch 2: Train Loss: 0.4122 Acc: 0.8262 | Val Loss: 0.5212 Acc: 0.7222\n","Epoch 3: Train Loss: 0.3852 Acc: 0.8405 | Val Loss: 0.5334 Acc: 0.7389\n","Epoch 4: Train Loss: 0.3541 Acc: 0.8357 | Val Loss: 0.5450 Acc: 0.7278\n","Epoch 5: Train Loss: 0.3518 Acc: 0.8429 | Val Loss: 0.5609 Acc: 0.7556\n","Epoch 6: Train Loss: 0.3140 Acc: 0.8810 | Val Loss: 0.5693 Acc: 0.7389\n","Epoch 7: Train Loss: 0.2711 Acc: 0.9000 | Val Loss: 0.5756 Acc: 0.7389\n","Epoch 8: Train Loss: 0.2627 Acc: 0.9167 | Val Loss: 0.6068 Acc: 0.7278\n","Epoch 9: Train Loss: 0.2460 Acc: 0.8976 | Val Loss: 0.6332 Acc: 0.7278\n","Epoch 10: Train Loss: 0.2310 Acc: 0.9071 | Val Loss: 0.6453 Acc: 0.7000\n"]}]},{"cell_type":"code","source":["torch.save(student_finetune.state_dict(), '/content/drive/MyDrive/mods/student_finetuned_3000_epoch.pth')"],"metadata":{"id":"RfyC4LRkAdjX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#SimCLR Pretraining 6 000 with compact function"],"metadata":{"id":"y-6jKftE9--R"}},{"cell_type":"code","source":["!pip install lightly\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764392469097,"user_tz":-420,"elapsed":9336,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"b6bb79c8-cbc0-4f5b-d48c-56e846d1c7bf","collapsed":true,"id":"Ah4dxQMg9--T"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightly\n","  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly) (2025.11.12)\n","Collecting hydra-core>=1.0.0 (from lightly)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting lightly_utils~=0.0.0 (from lightly)\n","  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.0.2)\n","Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0.post0)\n","Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.32.4)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly) (1.17.0)\n","Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from lightly) (0.24.0+cu126)\n","Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.12.3)\n","Collecting pytorch_lightning>=1.0.4 (from lightly)\n","  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.5.0)\n","Collecting aenum>=3.1.11 (from lightly)\n","  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (25.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from lightly_utils~=0.0.0->lightly) (11.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (2.41.4)\n","Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.4.2)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning>=1.0.4->lightly) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2025.3.0)\n","Collecting torchmetrics>0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (3.13.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->lightly) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->lightly) (3.0.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.22.0)\n","Downloading lightly-1.5.22-py3-none-any.whl (859 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n","Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: aenum, lightning-utilities, lightly_utils, hydra-core, torchmetrics, pytorch_lightning, lightly\n","Successfully installed aenum-3.1.16 hydra-core-1.3.2 lightly-1.5.22 lightly_utils-0.0.2 lightning-utilities-0.15.2 pytorch_lightning-2.6.0 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","metadata":{"id":"8e666a36"},"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","from lightly.transforms import SimCLRTransform\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from lightly.loss import NTXentLoss\n","\n","# Define the SimCLRModel class globally as it's a core component\n","class SimCLRModel(nn.Module):\n","    def __init__(self, backbone, feature_dim=128):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(1280, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, feature_dim)\n","        )\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        projections = self.projection_head(features)\n","        return projections\n","\n","def load_simclr_model(feature_dim=128, pretrained_path=None):\n","    \"\"\"\n","    Loads a MobileNetV2 backbone and constructs the SimCLR model.\n","    Optionally loads pretrained weights.\n","    \"\"\"\n","    # Load MobileNetV2 without pretrained weights and remove classifier\n","    base_model = mobilenet_v2(weights=None)\n","\n","    # Remove the classification head and keep only the feature extractor\n","    backbone = nn.Sequential(\n","        base_model.features,\n","        nn.AdaptiveAvgPool2d(1),  # Ensure consistent output shape\n","        nn.Flatten(),             # Shape: [B, 1280]\n","    )\n","\n","    model = SimCLRModel(backbone, feature_dim=feature_dim)\n","\n","    if pretrained_path:\n","        print(f\"Loading pretrained model from {pretrained_path}\")\n","        # Use map_location='cpu' to load onto CPU first, then move to device\n","        student_state_dict = torch.load(pretrained_path, map_location='cpu')\n","        model.load_state_dict(student_state_dict, strict=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","    print(\"SimCLR model loaded successfully.\")\n","    return model, device\n","\n","# SimCLR Transform from lightly\n","  # you can change input_size as needed\n","\n","# Custom dataset to return 2 views\n","class SimCLRDataset(ImageFolder):\n","    def __getitem__(self, index):\n","        simclr_transform = SimCLRTransform(input_size=224)\n","        sample, _ = super().__getitem__(index)\n","        xi, xj = simclr_transform(sample)\n","        return xi, xj\n","\n","def load_simclr_data(data_path, batch_size=64, input_size=224, num_workers=2):\n","    \"\"\"\n","    Loads the dataset and creates a DataLoader for SimCLR pretraining.\n","    \"\"\"\n","    simclr_transform = SimCLRTransform(input_size=input_size)\n","    dataset = SimCLRDataset(root=data_path)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n","    num_imgs = len(dataset)\n","    print(f\"Number of images in the dataset: {num_imgs}\")\n","    return dataloader, num_imgs\n","\n","def train_simclr_model(model, dataloader, device, epochs=20, lr=3e-4, save_best_path=None, save_epoch_path=None, start_epoch=0, initial_mloss=float('inf')):\n","    \"\"\"\n","    Trains the SimCLR model.\n","    \"\"\"\n","    criterion = NTXentLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    mloss = initial_mloss # Keep track of the minimum loss for saving the best model\n","\n","    print(\"Starting SimCLR training...\")\n","    for epoch in range(start_epoch, start_epoch + epochs):\n","        running_loss = 0.0\n","        total_batches = 0\n","        for views in dataloader:\n","            view1, view2 = views[0].to(device), views[1].to(device)\n","\n","            optimizer.zero_grad()\n","            z1 = model(view1)\n","            z2 = model(view2)\n","            loss = criterion(z1, z2)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            total_batches += 1\n","\n","        avg_epoch_loss = running_loss / total_batches if total_batches > 0 else 0.0\n","\n","        print(f\"Epoch {epoch + 1}: Loss = {avg_epoch_loss:.4f}\")\n","\n","        # Save model if current loss is the best so far\n","        if save_best_path and avg_epoch_loss < mloss:\n","            mloss = avg_epoch_loss\n","            torch.save(model.state_dict(), save_best_path)\n","            print(f\"Saved best model with loss {mloss:.4f} at epoch {epoch + 1}\")\n","\n","        # Save model periodically\n","        if save_epoch_path and (epoch + 1) % 2 == 0:\n","            torch.save(model.state_dict(), save_epoch_path)\n","            print(f\"Saved model checkpoint at epoch {epoch + 1}\")\n","\n","    print(\"SimCLR training finished.\")\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model, device = load_simclr_model(feature_dim=128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZyHJ-FR_K_T","executionInfo":{"status":"ok","timestamp":1764392713137,"user_tz":-420,"elapsed":451,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"b52571ba-655d-4f31-906c-038f2ce05cf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SimCLR model loaded successfully.\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/pets0/train6000'\n","dataloader, num_imgs = load_simclr_data(data_path, batch_size=64, input_size=224, num_workers=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SrXsVzJ_Xgb","executionInfo":{"status":"ok","timestamp":1764393437487,"user_tz":-420,"elapsed":105,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"dcd55be9-b743-416c-8612-cbd8e69d315b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in the dataset: 6000\n"]}]},{"cell_type":"code","source":["model = train_simclr_model(\n","     model, dataloader, device, epochs=20, lr=3e-4,\n","     save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","     save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth',\n","     start_epoch=0 # For new training\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jh57v2YT_gxl","executionInfo":{"status":"ok","timestamp":1764396396118,"user_tz":-420,"elapsed":2956673,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"23b81794-e185-4c34-e600-f197ad10158a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting SimCLR training...\n","Epoch 1: Loss = 4.8232\n","Saved best model with loss 4.8232 at epoch 1\n","Epoch 2: Loss = 4.7587\n","Saved best model with loss 4.7587 at epoch 2\n","Saved model checkpoint at epoch 2\n","Epoch 3: Loss = 4.6996\n","Saved best model with loss 4.6996 at epoch 3\n","Epoch 4: Loss = 4.6325\n","Saved best model with loss 4.6325 at epoch 4\n","Saved model checkpoint at epoch 4\n","Epoch 5: Loss = 4.5592\n","Saved best model with loss 4.5592 at epoch 5\n","Epoch 6: Loss = 4.4771\n","Saved best model with loss 4.4771 at epoch 6\n","Saved model checkpoint at epoch 6\n","Epoch 7: Loss = 4.3855\n","Saved best model with loss 4.3855 at epoch 7\n","Epoch 8: Loss = 4.3165\n","Saved best model with loss 4.3165 at epoch 8\n","Saved model checkpoint at epoch 8\n","Epoch 9: Loss = 4.2478\n","Saved best model with loss 4.2478 at epoch 9\n","Epoch 10: Loss = 4.2240\n","Saved best model with loss 4.2240 at epoch 10\n","Saved model checkpoint at epoch 10\n","Epoch 11: Loss = 4.1939\n","Saved best model with loss 4.1939 at epoch 11\n","Epoch 12: Loss = 4.1348\n","Saved best model with loss 4.1348 at epoch 12\n","Saved model checkpoint at epoch 12\n","Epoch 13: Loss = 4.1152\n","Saved best model with loss 4.1152 at epoch 13\n","Epoch 14: Loss = 4.0943\n","Saved best model with loss 4.0943 at epoch 14\n","Saved model checkpoint at epoch 14\n","Epoch 15: Loss = 4.0729\n","Saved best model with loss 4.0729 at epoch 15\n","Epoch 16: Loss = 4.0569\n","Saved best model with loss 4.0569 at epoch 16\n","Saved model checkpoint at epoch 16\n","Epoch 17: Loss = 4.0356\n","Saved best model with loss 4.0356 at epoch 17\n","Epoch 18: Loss = 4.0112\n","Saved best model with loss 4.0112 at epoch 18\n","Saved model checkpoint at epoch 18\n","Epoch 19: Loss = 4.0145\n","Epoch 20: Loss = 3.9897\n","Saved best model with loss 3.9897 at epoch 20\n","Saved model checkpoint at epoch 20\n","SimCLR training finished.\n"]}]},{"cell_type":"code","source":["model = train_simclr_model(\n","     model, dataloader, device, epochs=10, lr=3e-4,\n","     save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","     save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth',\n","     start_epoch=21,\n","     initial_mloss = 3.9897\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJPWynDEOb8R","executionInfo":{"status":"ok","timestamp":1764397782985,"user_tz":-420,"elapsed":1187762,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"7738d116-79d9-4451-f69c-d44abdb77353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting SimCLR training...\n","Epoch 22: Loss = 3.9808\n","Saved best model with loss 3.9808 at epoch 22\n","Saved model checkpoint at epoch 22\n","Epoch 23: Loss = 3.9529\n","Saved best model with loss 3.9529 at epoch 23\n","Epoch 24: Loss = 3.9589\n","Saved model checkpoint at epoch 24\n","Epoch 25: Loss = 3.9534\n","Epoch 26: Loss = 3.9527\n","Saved best model with loss 3.9527 at epoch 26\n","Saved model checkpoint at epoch 26\n","Epoch 27: Loss = 3.9244\n","Saved best model with loss 3.9244 at epoch 27\n","Epoch 28: Loss = 3.9112\n","Saved best model with loss 3.9112 at epoch 28\n","Saved model checkpoint at epoch 28\n","Epoch 29: Loss = 3.9094\n","Saved best model with loss 3.9094 at epoch 29\n","Epoch 30: Loss = 3.8954\n","Saved best model with loss 3.8954 at epoch 30\n","Saved model checkpoint at epoch 30\n","Epoch 31: Loss = 3.8855\n","Saved best model with loss 3.8855 at epoch 31\n","SimCLR training finished.\n"]}]},{"cell_type":"code","source":["model = train_simclr_model(\n","     model, dataloader, device, epochs=10, lr=3e-4,\n","     save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","     save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth',\n","     start_epoch=32,\n","     initial_mloss = 3.8855\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cQ6avzF8Tgqz","executionInfo":{"status":"ok","timestamp":1764399061436,"user_tz":-420,"elapsed":1219914,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"cb6798e7-5916-43b3-b81c-c66c48ba20a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting SimCLR training...\n","Epoch 33: Loss = 3.8926\n","Epoch 34: Loss = 3.8753\n","Saved best model with loss 3.8753 at epoch 34\n","Saved model checkpoint at epoch 34\n","Epoch 35: Loss = 3.8603\n","Saved best model with loss 3.8603 at epoch 35\n","Epoch 36: Loss = 3.8596\n","Saved best model with loss 3.8596 at epoch 36\n","Saved model checkpoint at epoch 36\n","Epoch 37: Loss = 3.8481\n","Saved best model with loss 3.8481 at epoch 37\n","Epoch 38: Loss = 3.8348\n","Saved best model with loss 3.8348 at epoch 38\n","Saved model checkpoint at epoch 38\n","Epoch 39: Loss = 3.8436\n","Epoch 40: Loss = 3.8275\n","Saved best model with loss 3.8275 at epoch 40\n","Saved model checkpoint at epoch 40\n","Epoch 41: Loss = 3.8259\n","Saved best model with loss 3.8259 at epoch 41\n","Epoch 42: Loss = 3.8065\n","Saved best model with loss 3.8065 at epoch 42\n","Saved model checkpoint at epoch 42\n","SimCLR training finished.\n"]}]},{"cell_type":"code","source":["model = train_simclr_model(\n","     model, dataloader, device, epochs=10, lr=3e-4,\n","     save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","     save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth',\n","     start_epoch=43,\n","     initial_mloss = 3.8065\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zdK8Kb5AWda6","outputId":"277b2243-b611-422f-ebd3-eea5bb6c1013"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting SimCLR training...\n","Epoch 44: Loss = 3.8167\n","Saved model checkpoint at epoch 44\n"]}]},{"cell_type":"markdown","source":["##Continue pretraining"],"metadata":{"id":"JesxE4zsyn7s"}},{"cell_type":"code","source":["# To resume training from a saved checkpoint:\n","pretrained_model_path = '/content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth'\n","model, device = load_simclr_model(feature_dim=128, pretrained_path=pretrained_model_path)"],"metadata":{"id":"-yje5udoJtV3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764409129606,"user_tz":-420,"elapsed":206,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"789e3825-93d0-481b-cd29-0f573f16bdd2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading pretrained model from /content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth\n","SimCLR model loaded successfully.\n"]}]},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/pets/train6000'\n","dataloader, num_imgs = load_simclr_data(data_path, batch_size=64, input_size=224, num_workers=2)"],"metadata":{"id":"-te7jXPFKc3O","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764409154659,"user_tz":-420,"elapsed":11550,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"fe163ab2-4ac7-420f-d01e-62d6f379d33e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in the dataset: 6000\n"]}]},{"cell_type":"code","source":["# Example for continuing training from a checkpoint (adjust start_epoch and initial_mloss based on previous runs)\n","model = train_simclr_model(\n","        model, dataloader, device, epochs=15, lr=3e-4,\n","        save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","        save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_6000_epoch.pth',\n","        start_epoch=45,\n","        initial_mloss=3.8065 # Based on the mloss from the last training run\n","    )"],"metadata":{"id":"grBnFTIOLNUk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764413080411,"user_tz":-420,"elapsed":3792148,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"9b55d883-5463-4e02-8c85-4643ac35205d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting SimCLR training...\n","Epoch 46: Loss = 3.8067\n","Saved model checkpoint at epoch 46\n","Epoch 47: Loss = 3.7897\n","Saved best model with loss 3.7897 at epoch 47\n","Epoch 48: Loss = 3.7713\n","Saved best model with loss 3.7713 at epoch 48\n","Saved model checkpoint at epoch 48\n","Epoch 49: Loss = 3.7787\n","Epoch 50: Loss = 3.7828\n","Saved model checkpoint at epoch 50\n","Epoch 51: Loss = 3.7639\n","Saved best model with loss 3.7639 at epoch 51\n","Epoch 52: Loss = 3.7698\n","Saved model checkpoint at epoch 52\n","Epoch 53: Loss = 3.7530\n","Saved best model with loss 3.7530 at epoch 53\n","Epoch 54: Loss = 3.7421\n","Saved best model with loss 3.7421 at epoch 54\n","Saved model checkpoint at epoch 54\n","Epoch 55: Loss = 3.7503\n","Epoch 56: Loss = 3.7428\n","Saved model checkpoint at epoch 56\n","Epoch 57: Loss = 3.7253\n","Saved best model with loss 3.7253 at epoch 57\n","Epoch 58: Loss = 3.7294\n","Saved model checkpoint at epoch 58\n","Epoch 59: Loss = 3.7302\n","Epoch 60: Loss = 3.7256\n","Saved model checkpoint at epoch 60\n","SimCLR training finished.\n"]}]},{"cell_type":"markdown","source":["#Finetune the simCLR model 6000 with compact function"],"metadata":{"id":"XsohCi0ZLenF"}},{"cell_type":"code","metadata":{"id":"206fe552"},"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def finetune_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, save_best=None, save_epoch=None, start_epoch=0, init_acc = 0.0):\n","    best_val_acc = init_acc\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","        epoch_loss = running_loss / total\n","        epoch_acc = correct / total\n","\n","        # Validation\n","        model.eval()\n","        val_running_loss = 0.0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                val_running_loss += loss.item() * images.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_correct += (preds == labels).sum().item()\n","                val_total += labels.size(0)\n","\n","        val_loss = val_running_loss / val_total\n","        val_acc = val_correct / val_total\n","\n","        print(f'Epoch {start_epoch + epoch + 1}: '\n","              f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n","\n","        if save_epoch and (epoch+1)%2==0:\n","            torch.save(model.state_dict(), save_epoch)\n","            print(f\"Saved model checkpoint at epoch {start_epoch + epoch + 1}\")\n","\n","        if save_best:\n","            if val_acc > best_val_acc:\n","                best_val_acc = val_acc\n","                torch.save(model.state_dict(), save_best)\n","                print(f\"Saved best model with Val Acc: {best_val_acc:.4f} at epoch {start_epoch + epoch + 1}\")\n","\n","    return best_val_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e25c8479"},"source":["def run_finetuning_workflow(pretrained_simclr_path=None, num_epochs_initial=10, best_save=None,\n","                            save_epoch=None, start_epoch=0, init_acc = 0.0, finetuned_path=None):\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Create a standard MobileNetV2 model\n","    student_finetune = mobilenet_v2(weights=None)\n","    num_ftrs = student_finetune.classifier[1].in_features\n","    student_finetune.classifier[1] = nn.Linear(num_ftrs, 2) # 2 classes: cat, dog\n","\n","    if finetuned_path:\n","        print(f\"Continuing finetuning from {finetuned_path}\")\n","        student_finetune.load_state_dict(torch.load(finetuned_path, map_location=device))\n","        print(\"Previous finetuned model loaded successfully.\")\n","    elif pretrained_simclr_path:\n","        print(f\"Starting new finetuning using SimCLR backbone from {pretrained_simclr_path}\")\n","        # Load the pretrained SimCLR model state dict\n","        simclr_state_dict = torch.load(pretrained_simclr_path, map_location=device)\n","\n","        # Filter the state dict to keep only the backbone weights\n","        backbone_state_dict = {}\n","        for k, v in simclr_state_dict.items():\n","            # Keys in the saved SimCLRModel state dict for the backbone start with 'backbone.0.'\n","            if k.startswith('backbone.0.'):\n","                backbone_state_dict[k.replace('backbone.0.', 'features.')] = v\n","            # Also handle the case if the state dict keys were just 'backbone.' without the '0.'\n","            elif k.startswith('backbone.'):\n","                backbone_state_dict[k.replace('backbone.', 'features.')] = v\n","            # Handle projection head weights if directly loading the SimCLRModel's state_dict\n","            elif k.startswith('projection_head.'):\n","                # These are not needed for finetuning the classification head, so we ignore them\n","                pass\n","\n","\n","        # Load the backbone weights into the standard MobileNetV2 model\n","        # Use strict=False because we are not loading the classifier weights\n","        student_finetune.load_state_dict(backbone_state_dict, strict=False)\n","        print(\"Pretrained SimCLR backbone loaded and classifier replaced.\")\n","    else:\n","        raise ValueError(\"Either pretrained_simclr_path must be provided for new finetuning, or cont=True and finetuned_path must be provided for resuming.\")\n","\n","    student_finetune = student_finetune.to(device)\n","\n","    # Define transforms for finetuning\n","    finetune_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomRotation(15),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    val_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.CenterCrop(192),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    # Create datasets and dataloaders for finetuning\n","    finetune_dataset_labeled = datasets.ImageFolder('/content/drive/MyDrive/pets/finetune_train', transform=finetune_transform)\n","    val_dataset_labeled = datasets.ImageFolder('/content/drive/MyDrive/pets/val', transform=val_transform)\n","\n","    finetune_loader_labeled = DataLoader(finetune_dataset_labeled, batch_size=64, shuffle=True, num_workers=2)\n","    val_loader_labeled = DataLoader(val_dataset_labeled, batch_size=64, shuffle=False, num_workers=2)\n","\n","    print(f\"Number of samples in finetune dataset: {len(finetune_dataset_labeled)}\")\n","    print(f\"Number of samples in validation dataset: {len(val_dataset_labeled)}\")\n","\n","    # Define optimizer and loss function for finetuning\n","    optimizer_finetune = torch.optim.Adam(student_finetune.parameters(), lr=1e-4) # Start with a lower learning rate\n","    criterion_finetune = nn.CrossEntropyLoss()\n","\n","    # Initial finetuning\n","    finetune_model(\n","        student_finetune,\n","        finetune_loader_labeled,\n","        val_loader_labeled,\n","        optimizer_finetune,\n","        criterion_finetune,\n","        num_epochs=num_epochs_initial,\n","        device=device,\n","        save_best=best_save,\n","        save_epoch=save_epoch,\n","        start_epoch=start_epoch,\n","        init_acc=init_acc\n","    )\n","\n","    print(f\"Finetuning complete. Best model saved to {best_save}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["run_finetuning_workflow(pretrained_simclr_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","                        num_epochs_initial=20,\n","                        best_save='/content/drive/MyDrive/mods/student_finetuned_6000.pth',\n","                        save_epoch= '/content/drive/MyDrive/mods/student_finetuned_6000_epoch.pth'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W0sbWQA_lRkn","executionInfo":{"status":"ok","timestamp":1764421729813,"user_tz":-420,"elapsed":2231320,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"b8c5270b-2f93-4d45-f9f7-677438764c52"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting new finetuning using SimCLR backbone from /content/drive/MyDrive/mods/mobilenet_sim_6000.pth\n","Pretrained SimCLR backbone loaded and classifier replaced.\n","Number of samples in finetune dataset: 420\n","Number of samples in validation dataset: 180\n","Epoch 1: Train Loss: 0.6830 Acc: 0.5762 | Val Loss: 0.6445 Acc: 0.7000\n","Saved best model with Val Acc: 0.7000 at epoch 1\n","Epoch 2: Train Loss: 0.6348 Acc: 0.6762 | Val Loss: 0.5817 Acc: 0.7111\n","Saved model checkpoint at epoch 2\n","Saved best model with Val Acc: 0.7111 at epoch 2\n","Epoch 3: Train Loss: 0.5920 Acc: 0.7286 | Val Loss: 0.5371 Acc: 0.7333\n","Saved best model with Val Acc: 0.7333 at epoch 3\n","Epoch 4: Train Loss: 0.5626 Acc: 0.6952 | Val Loss: 0.5051 Acc: 0.7833\n","Saved model checkpoint at epoch 4\n","Saved best model with Val Acc: 0.7833 at epoch 4\n","Epoch 5: Train Loss: 0.5250 Acc: 0.7429 | Val Loss: 0.4874 Acc: 0.7944\n","Saved best model with Val Acc: 0.7944 at epoch 5\n","Epoch 6: Train Loss: 0.4930 Acc: 0.7690 | Val Loss: 0.4752 Acc: 0.7889\n","Saved model checkpoint at epoch 6\n","Epoch 7: Train Loss: 0.4647 Acc: 0.7952 | Val Loss: 0.4721 Acc: 0.7722\n","Epoch 8: Train Loss: 0.4642 Acc: 0.7786 | Val Loss: 0.5045 Acc: 0.7389\n","Saved model checkpoint at epoch 8\n","Epoch 9: Train Loss: 0.4122 Acc: 0.8238 | Val Loss: 0.5064 Acc: 0.7333\n","Epoch 10: Train Loss: 0.3939 Acc: 0.8238 | Val Loss: 0.4868 Acc: 0.7389\n","Saved model checkpoint at epoch 10\n","Epoch 11: Train Loss: 0.3621 Acc: 0.8405 | Val Loss: 0.4740 Acc: 0.7611\n","Epoch 12: Train Loss: 0.3554 Acc: 0.8429 | Val Loss: 0.5107 Acc: 0.7444\n","Saved model checkpoint at epoch 12\n","Epoch 13: Train Loss: 0.3159 Acc: 0.8595 | Val Loss: 0.5322 Acc: 0.7444\n","Epoch 14: Train Loss: 0.3172 Acc: 0.8500 | Val Loss: 0.5194 Acc: 0.7778\n","Saved model checkpoint at epoch 14\n","Epoch 15: Train Loss: 0.2837 Acc: 0.8881 | Val Loss: 0.5190 Acc: 0.7778\n","Epoch 16: Train Loss: 0.2808 Acc: 0.8857 | Val Loss: 0.5383 Acc: 0.7444\n","Saved model checkpoint at epoch 16\n","Epoch 17: Train Loss: 0.2652 Acc: 0.8857 | Val Loss: 0.5492 Acc: 0.7778\n","Epoch 18: Train Loss: 0.2194 Acc: 0.9095 | Val Loss: 0.5746 Acc: 0.7722\n","Saved model checkpoint at epoch 18\n","Epoch 19: Train Loss: 0.2317 Acc: 0.9024 | Val Loss: 0.5811 Acc: 0.7500\n","Epoch 20: Train Loss: 0.2066 Acc: 0.9143 | Val Loss: 0.5748 Acc: 0.7722\n","Saved model checkpoint at epoch 20\n","Finetuning complete. Best model saved to /content/drive/MyDrive/mods/student_finetuned_6000.pth\n"]}]}]}