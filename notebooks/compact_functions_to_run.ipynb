{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1meN5lAIy7tCiCe9_Ie1cwJOnZQe1D3dd","authorship_tag":"ABX9TyPAP8Fl42tmu4dp46UuOZ3E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#For accessing datasets and checkpoints in drive"],"metadata":{"id":"fk_LJWoqGI_H"}},{"cell_type":"code","source":["import os\n","import re\n","\n","PROJECT_PATH = '/content/CatDogCNN2'\n","DATA_PATH = f'{PROJECT_PATH}/pets0'\n","CKPT_PATH = f'{PROJECT_PATH}/mods'\n","\n","os.makedirs(DATA_PATH, exist_ok=True)\n","os.makedirs(CKPT_PATH, exist_ok=True)\n","\n","print(\"Project folders created:\")\n","print(PROJECT_PATH)\n","print(DATA_PATH)\n","print(CKPT_PATH)\n","\n","# =========================================\n","# 3. Download DATASET (pets0)\n","# =========================================\n","# Function to extract file ID from a Google Drive URL\n","def get_drive_id(url):\n","    match = re.search(r'file/d/([a-zA-Z0-9_-]+)', url)\n","    if match:\n","        return match.group(1)\n","    return None\n","\n","dataset_url = 'https://drive.google.com/file/d/1dUoT7hrUgzPOhvbt2qOwbvfaFIsXBijA/view?usp=drive_link'\n","dataset_id = get_drive_id(dataset_url)\n","dataset_zip = f\"{PROJECT_PATH}/pets0.zip\"\n","\n","if dataset_id:\n","    !gdown --id {dataset_id} -O {dataset_zip}\n","    !unzip -q {dataset_zip} -d {DATA_PATH}\n","    !rm {dataset_zip}\n","    print(\"Dataset ready!\")\n","else:\n","    print(\"Could not extract dataset ID from the URL.\")\n","\n","\n","# =========================================\n","# 4. Download CHECKPOINTS (mods)\n","# =========================================\n","checkpoint_url = 'https://drive.google.com/file/d/1cnI2-TXblTxdMwAFnic-QEJ-EiwX-QeD/view?usp=drive_link'\n","checkpoint_id = get_drive_id(checkpoint_url)\n","checkpoint_zip = f\"{PROJECT_PATH}/mods.zip\"\n","\n","if checkpoint_id:\n","    !gdown --id {checkpoint_id} -O {checkpoint_zip}\n","    !unzip -q {checkpoint_zip} -d {CKPT_PATH}\n","    !rm {checkpoint_zip}\n","    print(\"Checkpoints ready!\")\n","else:\n","    print(\"Could not extract checkpoint ID from the URL.\")\n"],"metadata":{"id":"KS-WewsDjOAC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764425436203,"user_tz":-420,"elapsed":55219,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"f0846778-5794-4b55-c79b-2b9786391ac7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Project folders created:\n","/content/CatDogCNN2\n","/content/CatDogCNN2/pets0\n","/content/CatDogCNN2/mods\n","/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1dUoT7hrUgzPOhvbt2qOwbvfaFIsXBijA\n","From (redirected): https://drive.google.com/uc?id=1dUoT7hrUgzPOhvbt2qOwbvfaFIsXBijA&confirm=t&uuid=aa16f096-9bb6-4a4e-b13b-5cd49a84c229\n","To: /content/CatDogCNN2/pets0.zip\n","100% 807M/807M [00:15<00:00, 53.1MB/s]\n","Dataset ready!\n","/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  warnings.warn(\n","Downloading...\n","From (original): https://drive.google.com/uc?id=1cnI2-TXblTxdMwAFnic-QEJ-EiwX-QeD\n","From (redirected): https://drive.google.com/uc?id=1cnI2-TXblTxdMwAFnic-QEJ-EiwX-QeD&confirm=t&uuid=486b0809-f263-4841-bca3-56ce9de9baab\n","To: /content/CatDogCNN2/mods.zip\n","100% 283M/283M [00:09<00:00, 31.0MB/s]\n","Checkpoints ready!\n"]}]},{"cell_type":"markdown","source":["Please pass the corresponding file paths to run the functions below."],"metadata":{"id":"KoURNIu8KAp5"}},{"cell_type":"markdown","source":["#SimCLR Pretraining\n"],"metadata":{"id":"y-6jKftE9--R"}},{"cell_type":"code","source":["!pip install lightly\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764392469097,"user_tz":-420,"elapsed":9336,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"b6bb79c8-cbc0-4f5b-d48c-56e846d1c7bf","collapsed":true,"id":"Ah4dxQMg9--T"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting lightly\n","  Downloading lightly-1.5.22-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from lightly) (2025.11.12)\n","Collecting hydra-core>=1.0.0 (from lightly)\n","  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n","Collecting lightly_utils~=0.0.0 (from lightly)\n","  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.0.2)\n","Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0.post0)\n","Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.32.4)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from lightly) (1.17.0)\n","Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.12/dist-packages (from lightly) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from lightly) (2.9.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from lightly) (0.24.0+cu126)\n","Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.12.3)\n","Collecting pytorch_lightning>=1.0.4 (from lightly)\n","  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.12/dist-packages (from lightly) (2.5.0)\n","Collecting aenum>=3.1.11 (from lightly)\n","  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (2.3.0)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core>=1.0.0->lightly) (25.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from lightly_utils~=0.0.0->lightly) (11.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (2.41.4)\n","Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.10.5->lightly) (0.4.2)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning>=1.0.4->lightly) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2025.3.0)\n","Collecting torchmetrics>0.7.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n","Collecting lightning-utilities>=0.10.0 (from pytorch_lightning>=1.0.4->lightly)\n","  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.27.0->lightly) (3.11)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->lightly) (3.5.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (3.13.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->lightly) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->lightly) (3.0.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning>=1.0.4->lightly) (1.22.0)\n","Downloading lightly-1.5.22-py3-none-any.whl (859 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n","Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n","Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: aenum, lightning-utilities, lightly_utils, hydra-core, torchmetrics, pytorch_lightning, lightly\n","Successfully installed aenum-3.1.16 hydra-core-1.3.2 lightly-1.5.22 lightly_utils-0.0.2 lightning-utilities-0.15.2 pytorch_lightning-2.6.0 torchmetrics-1.8.2\n"]}]},{"cell_type":"code","metadata":{"id":"64cd39ee"},"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","from lightly.transforms import SimCLRTransform\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","from lightly.loss import NTXentLoss\n","\n","# Define the SimCLRModel class globally as it's a core component\n","class SimCLRModel(nn.Module):\n","    def __init__(self, backbone, feature_dim=128):\n","        super().__init__()\n","        self.backbone = backbone\n","        self.projection_head = nn.Sequential(\n","            nn.Linear(1280, 256),\n","            nn.ReLU(),\n","            nn.Linear(256, feature_dim)\n","        )\n","\n","    def forward(self, x):\n","        features = self.backbone(x)\n","        projections = self.projection_head(features)\n","        return projections\n","\n","def load_simclr_model(feature_dim=128, pretrained_path=None):\n","    \"\"\"\n","    Loads a MobileNetV2 backbone and constructs the SimCLR model.\n","    Optionally loads pretrained weights.\n","    \"\"\"\n","    # Load MobileNetV2 without pretrained weights and remove classifier\n","    base_model = mobilenet_v2(weights=None)\n","\n","    # Remove the classification head and keep only the feature extractor\n","    backbone = nn.Sequential(\n","        base_model.features,\n","        nn.AdaptiveAvgPool2d(1),  # Ensure consistent output shape\n","        nn.Flatten(),             # Shape: [B, 1280]\n","    )\n","\n","    model = SimCLRModel(backbone, feature_dim=feature_dim)\n","\n","    if pretrained_path:\n","        print(f\"Loading pretrained model from {pretrained_path}\")\n","        # Use map_location='cpu' to load onto CPU first, then move to device\n","        student_state_dict = torch.load(pretrained_path, map_location='cpu')\n","        model.load_state_dict(student_state_dict, strict=True)\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","    print(\"SimCLR model loaded successfully.\")\n","    return model, device\n","\n","# Custom dataset to return 2 views for SimCLR\n","class SimCLRDataset(ImageFolder):\n","    def __init__(self, root, simclr_transform): # Changed `transform` to `simclr_transform` and removed passing it to super().__init__\n","        super().__init__(root) # Initialize ImageFolder without a transform\n","        self.simclr_transform = simclr_transform # Store the SimCLRTransform separately\n","\n","    def __getitem__(self, index):\n","        # ImageFolder returns (image, label), we only need the image for pretraining\n","        # Here, 'sample' will be the raw PIL image loaded by ImageFolder (since no transform was passed to super)\n","        sample, _ = super().__getitem__(index)\n","        xi, xj = self.simclr_transform(sample) # Apply SimCLR transform to get two views from the raw PIL image\n","        return xi, xj\n","\n","def load_simclr_data(data_path, batch_size=64, input_size=224, num_workers=2):\n","    \"\"\"\n","    Loads the dataset and creates a DataLoader for SimCLR pretraining.\n","    \"\"\"\n","    simclr_transform = SimCLRTransform(input_size=input_size)\n","    # Pass simclr_transform explicitly to our custom SimCLRDataset\n","    dataset = SimCLRDataset(root=data_path, simclr_transform=simclr_transform)\n","    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n","    num_imgs = len(dataset)\n","    print(f\"Number of images in the dataset: {num_imgs}\")\n","    return dataloader, num_imgs\n","\n","def train_simclr_model(model, dataloader, device, epochs=20, lr=3e-4, save_best_path=None, save_epoch_path=None, start_epoch=0, initial_mloss=float('inf')):\n","    \"\"\"\n","    Trains the SimCLR model.\n","    \"\"\"\n","    criterion = NTXentLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    mloss = initial_mloss # Keep track of the minimum loss for saving the best model\n","\n","    print(\"Starting SimCLR training...\")\n","    for epoch in range(start_epoch, start_epoch + epochs):\n","        running_loss = 0.0\n","        total_batches = 0\n","        for views in dataloader:\n","            view1, view2 = views[0].to(device), views[1].to(device)\n","\n","            optimizer.zero_grad()\n","            z1 = model(view1)\n","            z2 = model(view2)\n","            loss = criterion(z1, z2)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            total_batches += 1\n","\n","        avg_epoch_loss = running_loss / total_batches if total_batches > 0 else 0.0\n","\n","        print(f\"Epoch {epoch + 1}: Loss = {avg_epoch_loss:.4f}\")\n","\n","        # Save model if current loss is the best so far\n","        if save_best_path and avg_epoch_loss < mloss:\n","            mloss = avg_epoch_loss\n","            torch.save(model.state_dict(), save_best_path)\n","            print(f\"Saved best model with loss {mloss:.4f} at epoch {epoch + 1}\")\n","\n","        # Save model periodically (e.g., every 2 epochs or as specified)\n","        if save_epoch_path and (epoch + 1) % 2 == 0:\n","            torch.save(model.state_dict(), save_epoch_path)\n","            print(f\"Saved model checkpoint at epoch {epoch + 1}\")\n","\n","    print(\"SimCLR training finished.\")\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Load the model (and optionally resume training)"],"metadata":{"id":"E6DGvc12JzBA"}},{"cell_type":"code","source":["# To start a new training run:\n","model, device = load_simclr_model(feature_dim=128)\n","\n","# To resume training from a saved checkpoint:\n","pretrained_model_path = '/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch2.pth'\n","model, device = load_simclr_model(feature_dim=128, pretrained_path=pretrained_model_path)"],"metadata":{"id":"-yje5udoJtV3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Load the data"],"metadata":{"id":"GEqc-x2fKa8W"}},{"cell_type":"code","source":["data_path = '/content/drive/MyDrive/pets0/unlabeled_train'\n","dataloader, num_imgs = load_simclr_data(data_path, batch_size=64, input_size=224, num_workers=2)"],"metadata":{"id":"-te7jXPFKc3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train the model"],"metadata":{"id":"Kpq3aj76K_fT"}},{"cell_type":"code","source":["# Example for starting a new training from scratch (adjust epochs, lr, and save paths)\n","model = train_simclr_model(\n","         model, dataloader, device, epochs=50, lr=3e-4,\n","         save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_10000_best.pth',\n","         save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch.pth',\n","         start_epoch=0 # For new training\n","         )\n","\n"],"metadata":{"id":"1iZySq-2LBv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example for continuing training from a checkpoint (adjust start_epoch and initial_mloss based on previous runs)\n","    model = train_simclr_model(\n","        model, dataloader, device, epochs=10, lr=3e-4,\n","        save_best_path='/content/drive/MyDrive/mods/mobilenet_sim_10000_best_cont.pth',\n","        save_epoch_path='/content/drive/MyDrive/mods/mobilenet_sim_10000_epoch_cont.pth',\n","        start_epoch=68, # Assuming previous training ended at epoch 67 (58 + 10 epochs = 68)\n","        initial_mloss=1.97 # Based on the mloss from the last training run\n","    )"],"metadata":{"id":"grBnFTIOLNUk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Finetune the simCLR model"],"metadata":{"id":"XsohCi0ZLenF"}},{"cell_type":"code","metadata":{"id":"206fe552"},"source":["import torch\n","import torch.nn as nn\n","from torchvision.models import mobilenet_v2\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def finetune_model(model, train_loader, val_loader, optimizer, criterion, num_epochs, device, save_best=None, save_epoch=None, start_epoch=0, init_acc = 0.0):\n","    best_val_acc = init_acc\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","        epoch_loss = running_loss / total\n","        epoch_acc = correct / total\n","\n","        # Validation\n","        model.eval()\n","        val_running_loss = 0.0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                val_running_loss += loss.item() * images.size(0)\n","                _, preds = torch.max(outputs, 1)\n","                val_correct += (preds == labels).sum().item()\n","                val_total += labels.size(0)\n","\n","        val_loss = val_running_loss / val_total\n","        val_acc = val_correct / val_total\n","\n","        print(f'Epoch {start_epoch + epoch + 1}: '\n","              f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f} | '\n","              f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n","\n","        if save_epoch and (epoch+1)%2==0:\n","            torch.save(model.state_dict(), save_epoch)\n","            print(f\"Saved model checkpoint at epoch {start_epoch + epoch + 1}\")\n","\n","        if save_best:\n","            if val_acc > best_val_acc:\n","                best_val_acc = val_acc\n","                torch.save(model.state_dict(), save_best)\n","                print(f\"Saved best model with Val Acc: {best_val_acc:.4f} at epoch {start_epoch + epoch + 1}\")\n","\n","    return best_val_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e25c8479"},"source":["def run_finetuning_workflow(pretrained_simclr_path=None, num_epochs_initial=10, best_save=None,\n","                            save_epoch=None, start_epoch=0, init_acc = 0.0, finetuned_path=None):\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Create a standard MobileNetV2 model\n","    student_finetune = mobilenet_v2(weights=None)\n","    num_ftrs = student_finetune.classifier[1].in_features\n","    student_finetune.classifier[1] = nn.Linear(num_ftrs, 2) # 2 classes: cat, dog\n","\n","    if finetuned_path:\n","        print(f\"Continuing finetuning from {finetuned_path}\")\n","        student_finetune.load_state_dict(torch.load(finetuned_path, map_location=device))\n","        print(\"Previous finetuned model loaded successfully.\")\n","    elif pretrained_simclr_path:\n","        print(f\"Starting new finetuning using SimCLR backbone from {pretrained_simclr_path}\")\n","        # Load the pretrained SimCLR model state dict\n","        simclr_state_dict = torch.load(pretrained_simclr_path, map_location=device)\n","\n","        # Filter the state dict to keep only the backbone weights\n","        backbone_state_dict = {}\n","        for k, v in simclr_state_dict.items():\n","            # Keys in the saved SimCLRModel state dict for the backbone start with 'backbone.0.'\n","            if k.startswith('backbone.0.'):\n","                backbone_state_dict[k.replace('backbone.0.', 'features.')] = v\n","            # Also handle the case if the state dict keys were just 'backbone.' without the '0.'\n","            elif k.startswith('backbone.'):\n","                backbone_state_dict[k.replace('backbone.', 'features.')] = v\n","            # Handle projection head weights if directly loading the SimCLRModel's state_dict\n","            elif k.startswith('projection_head.'):\n","                # These are not needed for finetuning the classification head, so we ignore them\n","                pass\n","\n","\n","        # Load the backbone weights into the standard MobileNetV2 model\n","        # Use strict=False because we are not loading the classifier weights\n","        student_finetune.load_state_dict(backbone_state_dict, strict=False)\n","        print(\"Pretrained SimCLR backbone loaded and classifier replaced.\")\n","    else:\n","        raise ValueError(\"Either pretrained_simclr_path must be provided for new finetuning, or cont=True and finetuned_path must be provided for resuming.\")\n","\n","    student_finetune = student_finetune.to(device)\n","\n","    # Define transforms for finetuning\n","    finetune_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","        transforms.RandomRotation(15),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    val_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.CenterCrop(192),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    # Create datasets and dataloaders for finetuning\n","    finetune_dataset_labeled = datasets.ImageFolder('/content/drive/MyDrive/pets0/finetune_train', transform=finetune_transform)\n","    val_dataset_labeled = datasets.ImageFolder('/content/drive/MyDrive/pets0/val', transform=val_transform)\n","\n","    finetune_loader_labeled = DataLoader(finetune_dataset_labeled, batch_size=64, shuffle=True, num_workers=2)\n","    val_loader_labeled = DataLoader(val_dataset_labeled, batch_size=64, shuffle=False, num_workers=2)\n","\n","    print(f\"Number of samples in finetune dataset: {len(finetune_dataset_labeled)}\")\n","    print(f\"Number of samples in validation dataset: {len(val_dataset_labeled)}\")\n","\n","    # Define optimizer and loss function for finetuning\n","    optimizer_finetune = torch.optim.Adam(student_finetune.parameters(), lr=1e-4) # Start with a lower learning rate\n","    criterion_finetune = nn.CrossEntropyLoss()\n","\n","    # Initial finetuning\n","    finetune_model(\n","        student_finetune,\n","        finetune_loader_labeled,\n","        val_loader_labeled,\n","        optimizer_finetune,\n","        criterion_finetune,\n","        num_epochs=num_epochs_initial,\n","        device=device,\n","        save_best=best_save,\n","        save_epoch=save_epoch,\n","        start_epoch=start_epoch,\n","        init_acc=init_acc\n","    )\n","\n","    print(f\"Finetuning complete. Best model saved to {best_save}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ef52d0f6"},"source":["### Finetuning Run"]},{"cell_type":"code","metadata":{"id":"e36d39ca"},"source":["# Example for starting a new finetuning run:\n","run_finetuning_workflow(\n","    pretrained_simclr_path='/content/drive/MyDrive/mods/mobilenet_sim_6000.pth',\n","    num_epochs_initial=10,\n","    best_save='/content/drive/MyDrive/mods/student_finetuned_6000_new.pth',\n","    save_epoch='/content/drive/MyDrive/mods/student_finetuned_6000_epoch_new.pth'\n",")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example for continuing finetuning from a saved finetuned model:\n","run_finetuning_workflow(\n","\n","     finetuned_path='/content/drive/MyDrive/mods/student_finetuned_6000_epoch_new.pth', # Path to a previously finetuned model\n","     num_epochs_initial=5,\n","     best_save='/content/drive/MyDrive/mods/student_finetuned_6000_cont_best.pth',\n","     save_epoch='/content/drive/MyDrive/mods/student_finetuned_6000_cont_epoch.pth',\n","     start_epoch=10, # If previous run had 10 epochs, start from 10\n","     init_acc=0.7 # Initial best accuracy from the previous finetuning run\n"," )"],"metadata":{"id":"wuI4UmBymrRF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Test the models"],"metadata":{"id":"84Keot5vOWZ_"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torchvision.models import mobilenet_v2\n","from torch.utils.data import DataLoader\n","from torchvision import transforms, datasets\n","from sklearn.metrics import classification_report, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","test_transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","test_dataset = datasets.ImageFolder('/content/drive/MyDrive/CatDogCNN/pets0/test', transform=test_transform)\n","print(f\"Number of images in the test dataset: {len(test_dataset)}\")\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","def test_model(model, test_loader, device, test_dataset_classes):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = correct / total\n","    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n","\n","    print(classification_report(all_labels, all_preds, target_names=test_dataset_classes))\n","\n","    cm_finetuned = confusion_matrix(all_labels, all_preds)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm_finetuned, annot=True, fmt='d', xticklabels=test_dataset_classes, yticklabels=test_dataset_classes)\n","    plt.xlabel('Predicted')\n","    plt.ylabel('True')\n","    plt.title('Confusion Matrix for Finetuned Student Model')\n","    plt.show()\n","    return np.array(all_preds), np.array(all_labels)\n"],"metadata":{"id":"zVNFhIJ0N5xE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764400892620,"user_tz":-420,"elapsed":20748,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}},"outputId":"ee6d4329-a340-43e9-c611-f221524cc4f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in the test dataset: 5000\n"]}]},{"cell_type":"markdown","source":["**Test the students (finetuned/ distilled)**"],"metadata":{"id":"RmGRwMgDVAN8"}},{"cell_type":"code","source":["def load_student_model(model_path=None):\n","  student = models.mobilenet_v2(weights=None)\n","  num_classes = 2\n","  student.classifier[1] = nn.Linear(student.last_channel, num_classes)\n","  if model_path:\n","      student_state_dict = torch.load(model_path, map_location='cpu')\n","      student.load_state_dict(student_state_dict, strict=True)\n","  student_test.eval()\n","  print(\"MobileNetV2 student loaded for tesing.\")\n","  return student"],"metadata":{"id":"ZJ_Dp8i02Jk0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","best_student_finetuned_path = '/content/drive/MyDrive/CatDogCNN/mods/student_finetuned_3000.pth'\n","student_test = load_student_model(best_student_finetuned_path)\n","test_model(student_test, test_loader, device, test_dataset.classes)"],"metadata":{"id":"uBoumG5T2ujf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Test the teacher (finetuned) - only 1 checkpoint**"],"metadata":{"id":"R8xFROhmVD0V"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764057025660,"user_tz":-420,"elapsed":5890,"user":{"displayName":"Trang Ho","userId":"06685644430566655680"}},"outputId":"383cf855-3eb0-4e3f-f236-b036aa899229","id":"hxf7zT_G_Brn"},"source":["# Define the path to your saved finetuned teacher model checkpoint\n","finetuned_checkpoint_path = '/content/drive/MyDrive/mods/resnet_finetune_only.pth'\n","\n","# Load a standard ResNet50 model structure\n","teacher_model = models.resnet50(weights=None) # Load without pretrained ImageNet weights initially\n","\n","# Modify the final fully connected layer to match the number of classes\n","num_ftrs = teacher_model.fc.in_features\n","num_classes = 2  # Your model was finetuned for 2 classes (Cat/Dog)\n","teacher_model.fc = nn.Linear(num_ftrs, num_classes)\n","\n","\n","# Load the state dictionary from the saved finetuned teacher model checkpoint\n","# Using map_location='cpu' to load onto CPU first is safer, then move to device\n","teacher_state_dict = torch.load(finetuned_checkpoint_path, map_location='cpu')\n","\n","# Load the state dictionary into the standard ResNet50 model\n","# This should now work because the model structure matches the saved state_dict\n","teacher_model.load_state_dict(teacher_state_dict)\n","\n","# Set the teacher model to evaluation mode\n","teacher_model.eval()\n","\n","# Determine the device based on CUDA availability\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move the teacher model to the device\n","teacher_model = teacher_model.to(device)\n","\n","print(\"Finetuned teacher model loaded correctly for testing.\")\n","\n","test_model(teacher_model, test_loader, device, test_dataset.classes)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Finetuned teacher model loaded correctly for testing.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PstRBFIsOR6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Compact KD function"],"metadata":{"id":"gCZdwTfNnVBR"}},{"cell_type":"code","metadata":{"id":"4561489f","executionInfo":{"status":"ok","timestamp":1764422973724,"user_tz":-420,"elapsed":32,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, ConcatDataset\n","\n","def kd_loss(student_logits, teacher_logits, T):\n","    \"\"\"KL divergence loss for soft logits.\"\"\"\n","    p_s = F.log_softmax(student_logits / T, dim=1)\n","    p_t = F.softmax(teacher_logits / T, dim=1)\n","    return F.kl_div(p_s, p_t, reduction='batchmean') * (T * T)\n","\n","def train_distillation_epoch(student_model, teacher_model, dataloader, criterion_ce, criterion_kd, optimizer, T, device, alpha):\n","    student_model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        with torch.no_grad():\n","            teacher_logits = teacher_model(inputs)\n","        student_logits = student_model(inputs)\n","\n","        labeled_mask = (labels != -1)\n","        unlabeled_mask = (labels == -1)\n","\n","        ce_loss = criterion_ce(student_logits[labeled_mask], labels[labeled_mask]) if labeled_mask.sum() > 0 else 0\n","        kd_loss_val = criterion_kd(student_logits[unlabeled_mask], teacher_logits[unlabeled_mask], T) if unlabeled_mask.sum() > 0 else 0\n","\n","        loss = (1 - alpha) * ce_loss + alpha * kd_loss_val\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * inputs.size(0)\n","        if labeled_mask.sum() > 0:\n","            _, preds = torch.max(student_logits[labeled_mask], 1)\n","            correct += (preds == labels[labeled_mask]).sum().item()\n","            total += labeled_mask.sum().item()\n","\n","    epoch_loss = running_loss / len(dataloader.dataset)\n","    epoch_acc = correct / total if total > 0 else 0.0\n","    return epoch_loss, epoch_acc\n","\n","def validate(model, dataloader, criterion):\n","    model.eval()\n","    running_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    with torch.no_grad():\n","        for imgs, labels in dataloader:\n","            imgs, labels = imgs.to(device), labels.to(device)\n","            outputs = model(imgs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item() * imgs.size(0)\n","            _, preds = torch.max(outputs, 1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_loss = running_loss / total\n","    val_acc = correct / total\n","    return val_loss, val_acc\n","\n","def run_distillation(num_epochs, student_model, teacher_model_path, labeled_dir,\n","                     unlabeled_dir, val_dir, img_size=224, batch_size=64, learning_rate=3e-4,\n","                     T=5.0, alpha=0.7, start = 0, save_path=None, save_best=None, device=None):\n","    if device is None:\n","        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","    # --- Teacher Model Setup ---\n","    teacher_model = models.resnet50(weights=None)\n","    num_ftrs_teacher = teacher_model.fc.in_features\n","    teacher_model.fc = nn.Linear(num_ftrs_teacher, 2)\n","    teacher_state_dict = torch.load(teacher_model_path, map_location='cpu')\n","    teacher_model.load_state_dict(teacher_state_dict)\n","    teacher_model.eval()\n","    for param in teacher_model.parameters():\n","        param.requires_grad = False\n","    teacher_model = teacher_model.to(device)\n","\n","    # --- Student Model Setup ---\n","    # Assuming student model is already defined and passed as an argument\n","    student_model = student_model.to(device)\n","    optimizer_student = torch.optim.Adam(student_model.parameters(), lr=learning_rate)\n","    criterion_ce = nn.CrossEntropyLoss()\n","\n","    # --- DataLoaders Setup ---\n","    train_transform = transforms.Compose([\n","        transforms.RandomResizedCrop(img_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","    val_transform = transforms.Compose([\n","        transforms.Resize(img_size),\n","        transforms.CenterCrop(img_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                             std=[0.229, 0.224, 0.225])\n","    ])\n","\n","    labeled_dataset = datasets.ImageFolder(labeled_dir, transform=train_transform)\n","    unlabeled_dataset = datasets.ImageFolder(unlabeled_dir, transform=train_transform)\n","    unlabeled_dataset.samples = [(path, -1) for (path, _) in unlabeled_dataset.samples]\n","    combined_dataset = ConcatDataset([labeled_dataset, unlabeled_dataset])\n","    train_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    print(f\"Number of images in the labeled dataset: {len(labeled_dataset)}\")\n","    print(f\"Number of images in the unlabeled dataset: {len(unlabeled_dataset)}\")\n","    print(f\"Number of images in the val dataset: {len(val_dataset)}\")\n","\n","    # --- Training Loop ---\n","    best_val_acc = 0.0\n","    for epoch in range(num_epochs):\n","        train_loss_student, train_acc_student = train_distillation_epoch(\n","            student_model, teacher_model, train_loader, criterion_ce, kd_loss, optimizer_student, T, device, alpha\n","        )\n","        val_loss_student, val_acc_student = validate(student_model, val_loader, criterion_ce)\n","\n","        print(f'Epoch {epoch+1+start}: '\n","              f'Train Loss (Student): {train_loss_student:.4f} Acc (Labeled): {train_acc_student:.4f} | '\n","              f'Val Loss (Student): {val_loss_student:.4f} Acc: {val_acc_student:.4f}')\n","\n","        if save_path and (epoch+1)%2==0:\n","            torch.save(student_model.state_dict(), save_path)\n","            print(f\"Saved student model at epoch {epoch+1+start}\")\n","\n","        if save_best and val_acc_student > best_val_acc:\n","            best_val_acc = val_acc_student\n","            torch.save(student_model.state_dict(), save_best)\n","            print(f\"Saved best student model with validation accuracy: {best_val_acc:.4f}\")\n","\n","    return student_model"],"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision.models as models\n","import torch.nn as nn\n","\n","\n","def load_student_model(model_path=None):\n","  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","  student = models.mobilenet_v2(weights=None)\n","  num_classes = 2\n","  student.classifier[1] = nn.Linear(student.last_channel, num_classes)\n","  if model_path:\n","      student_state_dict = torch.load(model_path, map_location='cpu')\n","      student.load_state_dict(student_state_dict, strict=True)\n","  student = student.to(device)\n","  print(\"MobileNetV2 student model defined with classification head.\")\n","  return student\n"],"metadata":{"id":"4A8QRjwFplSE","executionInfo":{"status":"ok","timestamp":1764422980525,"user_tz":-420,"elapsed":6,"user":{"displayName":"TRANG HO HUYEN","userId":"08257358141863163429"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["student = load_student_model('/content/drive/MyDrive/CatDogCNN/mods/only_distilled_student_3000.pth')\n","teacher_model_path = '/content/drive/MyDrive/CatDogCNN/mods/resnet_finetune_only.pth'\n","labeled_dir = '/content/drive/MyDrive/CatDogCNN/pets0/finetune_train'\n","unlabeled_dir = '/content/drive/MyDrive/CatDogCNN/pets0/train3000'\n","val_dir = '/content/drive/MyDrive/CatDogCNN/pets0/val'\n","\n","run_distillation(5, student, teacher_model_path, labeled_dir,\n","                     unlabeled_dir, val_dir, img_size=96, batch_size=32, learning_rate=3e-4,\n","                     T=5.0, alpha=0.7, start=3, save_path=None, save_best=None, device=None)"],"metadata":{"id":"-WH3cmz8pdAj"},"execution_count":null,"outputs":[]}]}